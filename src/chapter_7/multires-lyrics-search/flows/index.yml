jtype: Flow                                                   # We define the flow used for indexing here
version: '1'                                                  # yml version
with:                                                         # Parameters for the flow
  workspace: $JINA_WORKSPACE                                  # Workspace folder
executors:                                                    # Now, define all the executors that are used
  - name: segmenter                                           # The first executor splits the input text into sentences which are stored as chunks in the original documents
    uses: 'jinahub+docker://Sentencizer'                      # The type of the executor is Sentencizer, we download it from the hub as a docker container
  - name: encoder                                             # Then, compute the embeddings of the sentences in this executor
    uses: 'jinahub+docker://TransformerTorchEncoder/v0.1'          # We use a TransformerTorchEncoder from the hub
    volumes: '~/.cache/huggingface:/root/.cache/huggingface'  # Mount the huggingface cache into the docker container
    uses_with:                                                # Override some parameters for the executor
      pooling_strategy: 'cls'                                 # This is the pooling strategy that is used by the encoder
      pretrained_model_name_or_path: distilbert-base-cased    # The ML model that is used
      max_length: 96                                          # Max length argument for the tokenizer
      device: 'cpu'                                           # Run the executor on CPU - For GPU, we would have to use another container!
      default_traversal_paths: ['c']                          # Compute the embeddings on the chunk level - the sentences created before
  - name: indexer                                             # Now, index the sentences and store them to disk.
    uses: 'jinahub://SimpleIndexer/old'                           # We use a simple indexer for that purpose (not in docker, but using source codes - there are some bugs with docker for this executor)
    uses_metas:                                               # Set some meta arguments for this executor
      workspace: $JINA_WORKSPACE                              # Define the workspace folder for the executor
    uses_with:                                                # Override parameters for the executor
      default_traversal_paths: ['c']                          # Store the sentences on disk - this means on chunk level
  - name: root_indexer                                        # Additionally to the sentences, we also need to store the original songs which are not split into sentences
    uses: 'jinahub+docker://LMDBStorage'                      # Therefore, we use a LMDBStorage indexer
    volumes: $JINA_WORKSPACE_MOUNT                            # Again, mount the workspace
    uses_with:                                                # Override some parameters for the LMDBStorage
      default_traversal_paths: ['r']                          # Now, we store the root documents, not the sentence chunks
    needs: [gateway]                                          # We can start this at the beginning - in parallel to the sentence flow
  - name: wait_both                                           # Now, we wait for both the root indexing and the sentence path to finish
    needs: [indexer, root_indexer]                            # Continue once these two executor are finished
